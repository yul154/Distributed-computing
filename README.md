# Distributed-computing
Basic concept and relevant framework

* 分布式理论

* 客户端如何访问这么多的服务？
  * API 网关
* 服务于服务之间如何通信？
  * 同步通信：
    * HTTP（Apache Http Client）
    * RPC（Dubbo 只支持 Java，Apache Thrift，gRPC）
  * 异步通信：
    * 消息队列
      * Kafka
      * RabbitMQ
      * RocketMQ
* 这么多服务，如何管理？
  * 服务治理
    * 服务注册与发现
      * 基于客户端的服务注册与发现（Apache Zookeeper）
      * 基于服务端的服务注册与发现（Netflix Eureka）
*  服务挂了，怎么办？
  *  重试机制
  *  服务熔断
  *  服务降级
  *  服务限流
----
# 分布式理论

##  CAP
> CAP理论是分布式系统、特别是分布式存储领域中被讨论的最多的理论。

其中C代表一致性 (Consistency)，A代表可用性 (Availability)，P代表分区容错性 (Partition tolerance)。CAP理论告诉我们C、A、P三者不能同时满足，最多只能满足其中两个。


### CAP 三选二 
* 一致性 (Consistency): 一个写操作返回成功，那么之后的读请求都必须读到这个新数据；如果返回失败，那么所有读操作都不能读到这个数据。所有节点访问同一份最新的数据。 
* 可用性 (Availability): 非故障的节点在合理的时间内返回合理的响应（不是错误或者超时的响应）。
* 分区容错性 (Partition tolerance): 能容忍网络分区，在网络断开的情况下，被分隔的节点仍能正常对外提供服务。

```
什么是网络分区？
分布式系统中，多个节点之前的网络本来是连通的，但是因为某些故障（比如部分节点网络出了问题）某些节点之间不连通了，整个网络就分成了几块区域，这就叫网络分区。
```



### 对CAP理论的理解
两个副本处于分区两侧，即两个副本之间的网络断开
*  如果允许其中一个副本更新，则会导致数据不一致，即丧失了C性质。 
*  如果为了保证一致性，将分区某一侧的副本设置为不可用，那么又丧失了A性质。 
*  除非两个副本可以互相通信，才能既保证C又保证A，这又会导致丧失P性质。


```
当发生网络分区的时候，如果我们要继续服务，那么强一致性和可用性只能 2 选 1。
也就是说当网络分区之后 P 是前提，决定了 P 之后才有 C 和 A 的选择。
也就是说分区容错性（Partition tolerance）我们是必须要实现的。
简而言之就是：CAP 理论中分区容错性 P 是一定要满足的，在此基础上，只能满足可用性 A 或者一致性 C。

```
因此，分布式系统理论上不可能选择 CA 架构，只能选择 CP 或者 AP 架构(在系统发生“分区”的情况下，CAP 理论只能满足 CP 或者 AP。要注意的是，这里的前提是系统发生了“分区”)
```
为啥不可能选择 CA 架构呢？ 举个例子：若系统出现“分区”，系统中的某个节点在进行写操作。为了保证 C， 必须要禁止其他节点的读写操作，这就和 A 发生冲突了。如果为了保证 A，其他节点的读写操作正常的话，那就和 C 发生冲突了。
```

ZooKeeper 保证的是 CP。 
* 任何时刻对 ZooKeeper 的读请求都能得到一致性的结果，
* 但是， ZooKeeper 不保证每次请求的可用性比如在 Leader 选举过程中或者半数以上的机器不可用的时候服务就是不可用的。

Eureka 保证的则是 AP。 
* Eureka 在设计的时候就是优先保证 A （可用性）。
* 在 Eureka 中不存在什么 Leader 节点，每个节点都是一样的、平等的。
* 因此 Eureka 不会像 ZooKeeper 那样出现选举过程中或者半数以上的机器不可用的时候服务就是不可用的情况。
* Eureka 保证即使大部分节点挂掉也不会影响正常提供服务，只要有一个节点是可用的就行了。只不过这个节点上的数据可能并不是最新的。

Nacos 不仅支持 CP 也支持 AP

-----
# BASE 理论
>  Basically Available（基本可用） 、Soft-state（软状态） 和 Eventually Consistent（最终一致性） 三个短语的缩写

核心思想是即使无法做到强一致性（Strong Consistency，CAP 的一致性就是强一致性），但应用可以采用适合的方式达到最终一致性（Eventual Consitency）。


* Basically Available（基本可用）分布式系统在出现不可预知故障的时候，允许损失部分可用性 
```
允许损失部分可用性呢？
* 响应时间上的损失: 正常情况下，处理用户请求需要 0.5s 返回结果，但是由于系统出现故障，处理用户请求的时间变为 3 s。 
* 系统功能上的损失：正常情况下，用户可以使用系统的全部功能，但是由于系统访问量突然剧增，系统的部分非核心功能无法使用。 #

```

* Soft state（软状态）允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时
* Eventually consistent（最终一致性）最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性 
```
实现最终一致性的具体方式
* 读时修复 : 在读取数据时，检测数据的不一致，进行修复。比如 Cassandra 的 Read Repair 实现，具体来说，在向 Cassandra 系统查询数据的时候，如果检测到不同节点 的副本数据不一致，系统就自动修复数据。 
* 写时修复 : 在写入数据，检测数据的不一致时，进行修复。比如 Cassandra 的 Hinted Handoff 实现。具体来说，Cassandra 集群的节点之间远程写数据的时候，如果写失败 就将数据缓存下来，然后定时重传，修复数据的不一致性。 
* 异步修复 : 这个是最常用的方式，通过定时对账检测副本数据的一致性，并修复。
```
----
# Paxos 算法和 Raft 算法

## Paxos

## Raft
> 它将一致性分解为多个子问题: Leader选举(Leader election)、日志同步(Log replication)、安全性(Safety)、日志压缩(Log compaction)、成员变更(Membership change)等。同时，Raft算法使用了更强的假设来减少了需要考虑的状态，使之变的易于理解和实现



### 角色
*  Leader: 接受客户端请求，并向Follower同步请求日志，当日志同步到大多数节点上后告诉Follower提交日志。 
*  Follower: 接受并持久化Leader同步的日志，在Leader告之日志可以提交之后，提交日志。 
*  Candidate: Leader选举过程中的临时角色。

Raft要求系统在任意时刻最多只有一个Leader，正常工作期间只有Leader和Followers。


Follower只响应其他服务器的请求。如果Follower超时没有收到Leader的消息，它会成为一个Candidate并且开始一次Leader选举。收到大多数服务器投票的Candidate会成为新的Leader。Leader在宕机之前会一直保持Leader的状态

Raft算法将时间分为一个个的任期(term)，每一个term的开始都是Leader选举。在成功选举Leader之后，Leader会在整个term内管理整个集群。如果Leader选举失败，该term就会因为没有Leader而结束。


### Raft算法子问题

raft一致性协议有两大核心：选主和复制日志。

* 选主：节点有追随者，候选者，领导者三种角色，领导者从候选者中选出，只要有一半以上的节点投票（所谓投票就是节点返回响应）便可以选出领导者，三者角色可以转换
* 复制日志：用于维护数据一致性。领导者更新数据后，向追随者发送响应，只有追随者把信息写入本地，才会向领导者反馈响应，超过半数追随者反馈响应，即更新成功。此时，领导者将成功写入反馈给客户端
